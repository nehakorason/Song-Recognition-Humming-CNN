{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2GLWX69oym1"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "from os.path import isfile, join\n",
        "import numpy as np\n",
        "import shutil\n",
        "from tensorflow import keras\n",
        "from pathlib import Path\n",
        "from IPython.display import display, Audio\n",
        "import subprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uR2aKQtUo4OE"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPgytio3pGzo"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8k3MZEcpNxz"
      },
      "outputs": [],
      "source": [
        "!cp -r \"../input/song-recognition-dataset\" ./"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecc-curipZR-"
      },
      "outputs": [],
      "source": [
        "data_directory = \"/content/gdrive/MyDrive/Malayalam_Songs\"\n",
        "songs_folder = \"songs\"\n",
        "noise_folder = \"noise\"\n",
        "\n",
        "songs_path = os.path.join(data_directory, songs_folder)\n",
        "noise_path = os.path.join(data_directory, noise_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-__J29YsFz1"
      },
      "outputs": [],
      "source": [
        "songs_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igOlZkLKsXGQ"
      },
      "outputs": [],
      "source": [
        "valid_split = 0.1\n",
        "\n",
        "shuffle_seed = 43\n",
        "\n",
        "sample_rate = 16000\n",
        "\n",
        "scale = 0.5\n",
        "batch_size = 128\n",
        "\n",
        "epochs = 30\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKHmFWtHso9o"
      },
      "outputs": [],
      "source": [
        "for folder in os.listdir(data_directory):\n",
        "    if os.path.isdir(os.path.join(data_directory, folder)):\n",
        "        if folder in [songs_folder, noise_folder]:\n",
        "\n",
        "            continue\n",
        "        elif folder in [\"other\", \"background_noise\"]:\n",
        "\n",
        "            shutil.move(\n",
        "                os.path.join(data_directory, folder),\n",
        "                os.path.join(noise_path, folder),\n",
        "            )\n",
        "        else:\n",
        "            shutil.move(\n",
        "                os.path.join(data_directory, folder),\n",
        "                os.path.join(songs_path, folder),\n",
        "            )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXt-S3lKtHV2"
      },
      "outputs": [],
      "source": [
        "noise_paths = []\n",
        "for subdir in os.listdir(noise_path):\n",
        "    subdir_path = Path(noise_path) / subdir\n",
        "    if os.path.isdir(subdir_path):\n",
        "        noise_paths += [\n",
        "            os.path.join(subdir_path, filepath)\n",
        "            for filepath in os.listdir(subdir_path)\n",
        "            if filepath.endswith(\".wav\")\n",
        "        ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_ODQpH4tlNK"
      },
      "outputs": [],
      "source": [
        "noise_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uESALMp4ts8Q"
      },
      "outputs": [],
      "source": [
        "command = (\n",
        "    \"for dir in `ls -1 \" + noise_path + \"`; do \"\n",
        "    \"for file in `ls -1 \" + noise_path + \"/$dir/*.wav`; do \"\n",
        "    \"sample_rate=`ffprobe -hide_banner -loglevel panic -show_streams \"\n",
        "    \"$file | grep sample_rate | cut -f2 -d=`; \"\n",
        "    \"if [ $sample_rate -ne 3 ]; then \"\n",
        "    \"ffmpeg -hide_banner -loglevel panic -y \"\n",
        "    \"-i $file -ar 3 temp.wav; \"\n",
        "    \"mv temp.wav $file; \"\n",
        "    \"fi; done; done\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OaGQ2e0ty_F"
      },
      "outputs": [],
      "source": [
        "os.system(command)\n",
        "def load_noise_sample(path):\n",
        "    sample, sampling_rate = tf.audio.decode_wav(\n",
        "        tf.io.read_file(path), desired_channels=1\n",
        "    )\n",
        "    if sampling_rate == sample_rate:\n",
        "        slices = int(sample.shape[0] / sample_rate)\n",
        "        sample = tf.split(sample[: slices * sample_rate], slices)\n",
        "        return sample\n",
        "    else:\n",
        "        print(\"Sampling rate for\",path, \"is incorrect\")\n",
        "        return None\n",
        "\n",
        "\n",
        "noises = []\n",
        "for path in noise_paths:\n",
        "    sample = load_noise_sample(path)\n",
        "    if sample:\n",
        "        noises.extend(sample)\n",
        "noises = tf.stack(noises)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlNxORyqt6pL"
      },
      "outputs": [],
      "source": [
        "def paths_and_labels_to_dataset(songs_paths, labels):\n",
        "    path_ds = tf.data.Dataset.from_tensor_slices(songs_paths)\n",
        "    songs_ds = path_ds.map(lambda x: path_to_songs(x))\n",
        "    label_ds = tf.data.Dataset.from_tensor_slices(labels)\n",
        "    return tf.data.Dataset.zip((songs_ds, label_ds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxNaxJBDt-fb"
      },
      "outputs": [],
      "source": [
        "def path_to_songs(path):\n",
        "    songs = tf.io.read_file(path)\n",
        "    songs, _ = tf.audio.decode_wav(songs, 1, sample_rate)\n",
        "    return songs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWGXT07Iuvgs"
      },
      "outputs": [],
      "source": [
        "def add_noise(songs, noises=None, scale=0.5):\n",
        "    if noises is not None:\n",
        "        tf_rnd = tf.random.uniform(\n",
        "            (tf.shape(songs)[0],), 0, noises.shape[0], dtype=tf.int32\n",
        "        )\n",
        "        noise = tf.gather(noises, tf_rnd, axis=0)\n",
        "\n",
        "        prop = tf.math.reduce_max(songs, axis=1) / tf.math.reduce_max(noise, axis=1)\n",
        "        prop = tf.repeat(tf.expand_dims(prop, axis=1), tf.shape(songs)[1], axis=1)\n",
        "\n",
        "        songs = songs + noise * prop * scale\n",
        "\n",
        "    return songs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36ARdeOWu2UD"
      },
      "outputs": [],
      "source": [
        "def songs_to_fft(songs):\n",
        "    songs = tf.squeeze(songs, axis=-1)\n",
        "    fft = tf.signal.fft(\n",
        "        tf.cast(tf.complex(real=songs, imag=tf.zeros_like(songs)), tf.complex64)\n",
        "    )\n",
        "    fft = tf.expand_dims(fft, axis=-1)\n",
        "\n",
        "    return tf.math.abs(fft[:, : (songs.shape[1] // 2), :])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQN1fnfCu5f2"
      },
      "outputs": [],
      "source": [
        "class_names = os.listdir(songs_path)\n",
        "print(class_names,)\n",
        "\n",
        "songs_paths = []\n",
        "labels = []\n",
        "for label, name in enumerate(class_names):\n",
        "    print(\"Songs:\",(name))\n",
        "    dir_path = Path(songs_path) / name\n",
        "    songs_sample_paths = [\n",
        "        os.path.join(dir_path, filepath)\n",
        "        for filepath in os.listdir(dir_path)\n",
        "        if filepath.endswith(\".wav\")\n",
        "    ]\n",
        "    songs_paths += songs_sample_paths\n",
        "    labels += [label] * len(songs_sample_paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVI9DTM0gC5J"
      },
      "outputs": [],
      "source": [
        "# Shuffle to generate random data\n",
        "rng = np.random.RandomState(shuffle_seed)\n",
        "rng.shuffle(songs_paths)\n",
        "rng = np.random.RandomState(shuffle_seed)\n",
        "rng.shuffle(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRt0lpmbgKV_"
      },
      "outputs": [],
      "source": [
        "# Split into training and validation\n",
        "num_val_samples = int(valid_split * len(songs_paths))\n",
        "train_songs_paths = songs_paths[:-num_val_samples]\n",
        "train_labels = labels[:-num_val_samples]\n",
        "\n",
        "\n",
        "valid_songs_paths = songs_paths[-num_val_samples:]\n",
        "valid_labels = labels[-num_val_samples:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6i8fPUWmgsRN"
      },
      "outputs": [],
      "source": [
        "# Create datasets, one for training and the other for validation\n",
        "train_ds = paths_and_labels_to_dataset(train_songs_paths, train_labels)\n",
        "train_ds = train_ds.shuffle(buffer_size=batch_size * 8, seed=shuffle_seed).batch(\n",
        "    batch_size\n",
        ")\n",
        "\n",
        "valid_ds = paths_and_labels_to_dataset(valid_songs_paths, valid_labels)\n",
        "valid_ds = valid_ds.shuffle(buffer_size=32 * 8, seed=shuffle_seed).batch(32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4hPZupgvWFG"
      },
      "outputs": [],
      "source": [
        "# Add noise to the training set\n",
        "# train_ds = train_ds.map(\n",
        "#      lambda x, y: (add_noise(x, noises, scale=scale), y),\n",
        "#      num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
        "#  )\n",
        "\n",
        "# Transform audio wave to the frequency domain using `audio_to_fft`\n",
        "train_ds = train_ds.map(\n",
        "    lambda x, y: (songs_to_fft(x), y), num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
        ")\n",
        "\n",
        "train_ds = train_ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "valid_ds = valid_ds.map(\n",
        "    lambda x, y: (songs_to_fft(x), y), num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
        ")\n",
        "valid_ds = valid_ds.prefetch(tf.data.experimental.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayEuuptSlPW-"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Conv1D\n",
        "# import tensorflow as tf\n",
        "# from tensorflow import keras\n",
        "# from tensorflow.keras.optimizers.legacy import Adam"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import plot_model\n"
      ],
      "metadata": {
        "id": "SJHlyfWYPiin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.utils import plot_model"
      ],
      "metadata": {
        "id": "G8Lm4_4YPm02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDhVr30rlVDc"
      },
      "outputs": [],
      "source": [
        "def residual_block(x, filters, conv_num = 3, activation = \"relu\"):\n",
        "    s = keras.layers.Conv1D(filters, 1, padding = \"same\")(x)\n",
        "\n",
        "    for i in range(conv_num - 1):\n",
        "        x = keras.layers.Conv1D(filters, 3, padding = \"same\")(x)\n",
        "        x = keras.layers.Activation(activation)(x)\n",
        "\n",
        "    x = keras.layers.Conv1D(filters, 3, padding = \"same\")(x)\n",
        "    x = keras.layers.Add()([x, s])\n",
        "    x = keras.layers.Activation(activation)(x)\n",
        "\n",
        "    return keras.layers.MaxPool1D(pool_size = 2, strides = 2)(x)\n",
        "\n",
        "def build_model(input_shape, num_classes):\n",
        "    inputs = keras.layers.Input(shape = input_shape, name = \"input\")\n",
        "\n",
        "    x = residual_block(inputs, 16, 2)\n",
        "    x = residual_block(inputs, 32, 2)\n",
        "    x = residual_block(inputs, 64, 3)\n",
        "    x = residual_block(inputs, 128, 3)\n",
        "    x = residual_block(inputs, 128, 3)\n",
        "    x = keras.layers.AveragePooling1D(pool_size=3, strides=3)(x)\n",
        "    x = keras.layers.Flatten()(x)\n",
        "    x = keras.layers.Dense(256, activation=\"relu\")(x)\n",
        "    x = keras.layers.Dense(128, activation=\"relu\")(x)\n",
        "\n",
        "    outputs = keras.layers.Dense(num_classes, activation = \"softmax\", name = \"output\")(x)\n",
        "\n",
        "    return keras.models.Model(inputs = inputs, outputs = outputs)\n",
        "\n",
        "model = build_model((sample_rate // 2, 1), len(class_names))\n",
        "\n",
        "plot_model(model, to_file='model_architecture.png', show_shapes=True, show_layer_names=True)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer=\"Adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "model_save_filename = \"model.h5\"\n",
        "\n",
        "earlystopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
        "\n",
        "mdlcheckpoint_cb = keras.callbacks.ModelCheckpoint(model_save_filename, monitor=\"val_accuracy\", save_best_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('model_architecture.png')"
      ],
      "metadata": {
        "id": "sgrkDi6x7-Zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLKcmI2qlmHR"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    train_ds,\n",
        "    epochs=epochs,\n",
        "    validation_data=valid_ds,\n",
        "    callbacks=[earlystopping_cb, mdlcheckpoint_cb],\n",
        ")\n",
        "\n",
        "model.save(\"model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"model.h5\")"
      ],
      "metadata": {
        "id": "fK9f57m9Xg1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5kHVHi6GBbTg"
      },
      "outputs": [],
      "source": [
        "print(\"Accuracy of model:\",model.evaluate(valid_ds))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "M_VlfQh1MSGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the validation dataset\n",
        "loss, accuracy = model.evaluate(valid_ds)\n",
        "\n",
        "# Make predictions on the validation dataset\n",
        "y_pred = model.predict(valid_ds)\n",
        "y_true = np.concatenate([y for x, y in valid_ds], axis=0)\n",
        "\n",
        "# Compute precision, recall, and F1-score\n",
        "# Compute precision, recall, and F1-score\n",
        "print(\"Classification Report:\")\n",
        "#print(classification_report(y_true, np.argmax(y_pred, axis=1), labels=np.arange(len(class_names)), target_names=class_names))\n",
        "print(classification_report(y_true, np.argmax(y_pred, axis=1), labels=np.arange(len(class_names)), target_names=class_names, zero_division=\"warn\"))\n",
        "\n"
      ],
      "metadata": {
        "id": "1EAm2gD1QPd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute ROC curve for each class\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "for i in range(len(class_names)):\n",
        "    fpr[i], tpr[i], _ = roc_curve((y_true == i).astype(int), y_pred[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "# Plot ROC curve for each class\n",
        "plt.figure(figsize=(8, 6))\n",
        "for i in range(len(class_names)):\n",
        "    plt.plot(fpr[i], tpr[i], label=f'{class_names[i]} (AUC = {roc_auc[i]:.2f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "HW3fhmybQo9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z--JVziQKEc3"
      },
      "outputs": [],
      "source": [
        "SAMPLES_TO_DISPLAY = 10\n",
        "\n",
        "test_ds = paths_and_labels_to_dataset(valid_songs_paths, valid_labels)\n",
        "test_ds = test_ds.shuffle(buffer_size=batch_size * 8, seed=shuffle_seed).batch(\n",
        "    batch_size\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bfE_XRQB7jw"
      },
      "outputs": [],
      "source": [
        "def predict(path, labels):\n",
        "    test = paths_and_labels_to_dataset(path, labels)\n",
        "\n",
        "    test = test.shuffle(buffer_size=batch_size * 8, seed=shuffle_seed).batch(\n",
        "        batch_size\n",
        "    )\n",
        "    test = test.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    # test = test.map(lambda x, y: (add_noise(x, noises, scale=scale), y))\n",
        "\n",
        "    for songs, labels in test.take(1):\n",
        "        ffts = songs_to_fft(songs)\n",
        "        y_pred = model.predict(ffts)\n",
        "        rnd = np.random.randint(0, 1, 1)\n",
        "        songs = songs.numpy()[rnd, :]\n",
        "        labels = labels.numpy()[rnd]\n",
        "        y_pred = np.argmax(y_pred, axis=-1)[rnd]\n",
        "\n",
        "        for index in range(1):\n",
        "            print(\n",
        "                \"Song:\\33{} {}\\33[0m\\tPredicted:\\33{} {}\\33[0m\".format(\n",
        "                    \"[92m\", class_names[y_pred[index]],\n",
        "                    \"[92m\", y_pred[index]\n",
        "                )\n",
        "            )\n",
        "\n",
        "            print(\"Song Predicted:\", class_names[y_pred[index]])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9sDkI82CDBc"
      },
      "outputs": [],
      "source": [
        "path = [\"/content/gdrive/MyDrive/Malayalam_denoised/songs/Karimizhi2.wav\"]\n",
        "labels = [\"unknown\"]\n",
        "predict(path, labels)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}